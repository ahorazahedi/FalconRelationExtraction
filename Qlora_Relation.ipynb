{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer , AutoModelForCausalLM , BitsAndBytesConfig\n",
    "\n",
    "MODEL_HF_ID = \"tiiuae/falcon-7b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True , \n",
    "    bnb_4bit_use_double_quant=True  , \n",
    "    bnb_4bit_quant_type=\"nf4\" , \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_HF_ID , trust_remote_code = True )\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_HF_ID , quantization_config = bnb_config , device_map=\"auto\" , trust_remote_code = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4544, out_features=65024, bias=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RWForCausalLM(\n",
       "  (transformer): RWModel(\n",
       "    (word_embeddings): Embedding(65024, 4544)\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): Attention(\n",
       "          (maybe_rotary): RotaryEmbedding()\n",
       "          (query_key_value): Linear4bit(in_features=4544, out_features=4672, bias=False)\n",
       "          (dense): Linear4bit(in_features=4544, out_features=4544, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (dense_h_to_4h): Linear4bit(in_features=4544, out_features=18176, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (dense_4h_to_h): Linear4bit(in_features=18176, out_features=4544, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): CastOutputToFloat(\n",
       "    (0): Linear(in_features=4544, out_features=65024, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16318464 || all params: 3625063296 || trainable%: 0.4501566639679441\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\" , \n",
    "     target_modules=[\n",
    "        \"query_key_value\",\n",
    "        \"dense\",\n",
    "        \"dense_h_to_4h\",\n",
    "        \"dense_4h_to_h\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/ahora/.cache/huggingface/datasets/csv/default-4caa2e3dddaa112f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206be1e48e0b4231989ccadc9b04d437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv', data_files={\n",
    "    \"train\": \"./Dataset/train.csv\" , \n",
    "    'test': \"./Dataset/test.csv\" , \n",
    "    'valid' : './Dataset/valid.csv'\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 0,\n",
       " 'Abstract': 'Catecholamine-depleting drugs (eg, reserpine) may have an additive effect when given with beta-blocking agents. Patients treated with TENORMIN plus a catecholamine depletor should therefore be closely observed for evidence of hypotension and/or marked bradycardia which may produce vertigo, syncope, or postural hypotension. Calcium channel blockers may also have an additive effect when given with TENORMIN . Beta blockers may exacerbate the rebound hypertension which can follow the withdrawal of clonidine. If the two drugs are coadministered, the beta blocker should be withdrawn several days before the gradual withdrawal of clonidine. If replacing clonidine by beta-blocker therapy, the introduction of beta blockers should be delayed for several days after clonidine administration has stopped. Concomitant use of prostaglandin synthase inhibiting drugs, eg, indomethacin, may decrease the hypotensive effects of beta blockers. Information on concurrent usage of atenolol and aspirin is limited. Data from several studies, ie, TIMI-II, ISIS-2, currently do not suggest any clinical interaction between aspirin and beta blockers in the acute myocardial infarction setting. While taking beta blockers, patients with a history of anaphylactic reaction to a variety of allergens may have a more severe reaction on repeated challenge, either accidental, diagnostic or therapeutic. Such patients may be unresponsive to the usual doses of epinephrine used to treat the allergic reaction.',\n",
       " 'Relations': ' ( reserpine , effect_DDI , beta-blocking agent ) -  ( Calcium channel blockers , effect_DDI , TENORMIN ) -  ( Beta blockers , effect_DDI , clonidine ) -  ( beta blocker , advise_DDI , clonidine ) -  ( beta blockers , advise_DDI , clonidine ) -  ( indomethacin , effect_DDI , beta blockers )'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ahora/.cache/huggingface/datasets/csv/default-4caa2e3dddaa112f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-7626c92e47fdcc1c.arrow\n",
      "Loading cached processed dataset at /home/ahora/.cache/huggingface/datasets/csv/default-4caa2e3dddaa112f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-45288de2ad65f53f.arrow\n",
      "Loading cached processed dataset at /home/ahora/.cache/huggingface/datasets/csv/default-4caa2e3dddaa112f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-1e1f40f73e403fa0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Catecholamine-depleting drugs (eg, reserpine) may have an additive effect '\n",
      " 'when given with beta-blocking agents. Patients treated with TENORMIN plus a '\n",
      " 'catecholamine depletor should therefore be closely observed for evidence of '\n",
      " 'hypotension and/or marked bradycardia which may produce vertigo, syncope, or '\n",
      " 'postural hypotension. Calcium channel blockers may also have an additive '\n",
      " 'effect when given with TENORMIN . Beta blockers may exacerbate the rebound '\n",
      " 'hypertension which can follow the withdrawal of clonidine. If the two drugs '\n",
      " 'are coadministered, the beta blocker should be withdrawn several days before '\n",
      " 'the gradual withdrawal of clonidine. If replacing clonidine by beta-blocker '\n",
      " 'therapy, the introduction of beta blockers should be delayed for several '\n",
      " 'days after clonidine administration has stopped. Concomitant use of '\n",
      " 'prostaglandin synthase inhibiting drugs, eg, indomethacin, may decrease the '\n",
      " 'hypotensive effects of beta blockers. Information on concurrent usage of '\n",
      " 'atenolol and aspirin is limited. Data from several studies, ie, TIMI-II, '\n",
      " 'ISIS-2, currently do not suggest any clinical interaction between aspirin '\n",
      " 'and beta blockers in the acute myocardial infarction setting. While taking '\n",
      " 'beta blockers, patients with a history of anaphylactic reaction to a variety '\n",
      " 'of allergens may have a more severe reaction on repeated challenge, either '\n",
      " 'accidental, diagnostic or therapeutic. Such patients may be unresponsive to '\n",
      " 'the usual doses of epinephrine used to treat the allergic reaction. ->:  ( '\n",
      " 'reserpine , effect_DDI , beta-blocking agent ) -  ( Calcium channel blockers '\n",
      " ', effect_DDI , TENORMIN ) -  ( Beta blockers , effect_DDI , clonidine ) -  ( '\n",
      " 'beta blocker , advise_DDI , clonidine ) -  ( beta blockers , advise_DDI , '\n",
      " 'clonidine ) -  ( indomethacin , effect_DDI , beta blockers )']\n",
      "['Antacids: Absorption of a single dose of Myfortic was decreased when '\n",
      " 'administered to 12 stable renal transplant patients also taking '\n",
      " 'magnesium-aluminum containing antacids (30 mL): the mean Cmax and AUC(0-t) '\n",
      " 'values for MPA were 25% and 37% lower, respectively, than when Myfortic was '\n",
      " 'administered alone under fasting conditions. It is recommended that Myfortic '\n",
      " 'and antacids not be administered simultaneously. Cyclosporine: When studied '\n",
      " 'in stable renal transplant patients, cyclosporine, USP (MODIFIED) '\n",
      " 'pharmacokinetics were unaffected by steady state dosing of Myfortic. '\n",
      " 'Acyclovir/Ganciclovir: may be taken with Myfortic; however, during the '\n",
      " 'period of treatment, physicians should monitor blood cell counts. Both '\n",
      " 'acyclovir/ganciclovir and MPAG concentrations are increased in the presence '\n",
      " 'of renal impairment, their coexistence may compete for tubular secretion and '\n",
      " 'further increase in the concentrations of the two. '\n",
      " 'Azathioprine/Mycophenolate Mofetil: Given that azathioprine and '\n",
      " 'mycophenolate mofetil inhibit purine metabolism, it is recommended that '\n",
      " 'Myfortic not be administered concomitantly with azathioprine or '\n",
      " 'mycophenolate mofetil. Cholestyramine and Drugs that Bind Bile Acids: These '\n",
      " 'drugs interrupt enterohepatic recirculation and reduce MPA exposure when '\n",
      " 'coadministered with mycophenolate mofetil. Therefore, do not administer '\n",
      " 'Myfortic with cholestyramine or other agents that may interfere with '\n",
      " 'enterohepatic recirculation or drugs that may bind bile acids, for example '\n",
      " 'bile acid sequestrates or oral activated charcoal, because of the potential '\n",
      " 'to reduce the efficacy of Myfortic. Oral Contraceptives: Given the different '\n",
      " 'metabolism of Myfortic and oral contraceptives, no drug interaction between '\n",
      " 'these two classes of drug is expected. However, in a drug-drug interaction '\n",
      " 'study, mean levonorgesterol AUC was decreased by 15% when coadministered '\n",
      " 'with mycophenolate mofetil. Therefore, it is recommended that oral '\n",
      " 'contraceptives are co- administered with Myfortic with caution and '\n",
      " 'additional birth control methods be considered. Live Vaccines: During '\n",
      " 'treatment with Myfortic, the use of live attenuated vaccines should be '\n",
      " 'avoided and patients should be advised that vaccinations may be less '\n",
      " 'effective. Influenza vaccination may be of value. Prescribers should refer '\n",
      " 'to national guidelines for influenza vaccination. Drugs that alter the '\n",
      " 'gastrointestinal flora may interact with Myfortic by disrupting '\n",
      " 'enterohepatic recirculation. Interference of MPAG hydrolysis may lead to '\n",
      " 'less MPA available for absorption. ->:  ( Myfortic , mechanism_DDI , '\n",
      " 'magnesium ) -  ( Myfortic , mechanism_DDI , aluminum ) -  ( Myfortic , '\n",
      " 'advise_DDI , antacids ) -  ( Myfortic , advise_DDI , azathioprine ) -  ( '\n",
      " 'Myfortic , advise_DDI , mycophenolate mofetil ) -  ( Myfortic , advise_DDI , '\n",
      " 'cholestyramine ) -  ( Myfortic , advise_DDI , activated charcoal ) -  ( '\n",
      " 'levonorgesterol , mechanism_DDI , mycophenolate mofetil ) -  ( '\n",
      " 'contraceptives , advise_DDI , Myfortic ) -  ( Myfortic , advise_DDI , live '\n",
      " 'attenuated vaccines )']\n",
      "['Isocarboxazid should be administered with caution to patients receiving '\n",
      " 'Antabuse (disulfiram, Wyeth-Ayerst Laboratories). In a single study, rats '\n",
      " 'given high intraperitoneal doses of an MAO inhibitor plus disulfiram '\n",
      " 'experienced severe toxicity, including convulsions and death. Concomitant '\n",
      " 'use of Isocarboxazid and other psychotropic agents is generally not '\n",
      " 'recommended because of possible potentiating effects. This is especially '\n",
      " 'true in patients who may subject themselves to an overdosage of drugs. If '\n",
      " 'combination therapy is needed, careful consideration should be given to the '\n",
      " 'pharmacology of all agents to be used. The monoamine oxidase inhibitory '\n",
      " 'effects of Isocarboxazid may persist for a substantial period after '\n",
      " 'discontinuation of the drug, and this should be borne in mind when another '\n",
      " 'drug is prescribed following Isocarboxazid. To avoid potentiation, the '\n",
      " 'physician wishing to terminate treatment with Isocarboxazid and begin '\n",
      " 'therapy with another agent should allow for an interval of 10 days. ->:  ( '\n",
      " 'Isocarboxazid , advise_DDI , Antabuse ) -  ( Isocarboxazid , advise_DDI , '\n",
      " 'disulfiram ) -  ( MAO inhibitor , effect_DDI , disulfiram ) -  ( '\n",
      " 'Isocarboxazid , advise_DDI , psychotropic agents )']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "def merge_columns(example):\n",
    "    example[\"prediction\"] = example[\"Abstract\"] + \" ->: \" + str(example[\"Relations\"])\n",
    "    return example\n",
    "\n",
    "dataset['train'] = dataset['train'].map(merge_columns)\n",
    "dataset['test'] = dataset['test'].map(merge_columns)\n",
    "dataset['valid'] = dataset['valid'].map(merge_columns)\n",
    "pprint(dataset['train'][\"prediction\"][:1])\n",
    "pprint(dataset['test'][\"prediction\"][:1])\n",
    "pprint(dataset['valid'][\"prediction\"][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ahora/.cache/huggingface/datasets/csv/default-4caa2e3dddaa112f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-f61f1de940ffe691.arrow\n",
      "Loading cached processed dataset at /home/ahora/.cache/huggingface/datasets/csv/default-4caa2e3dddaa112f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-9bcb954d79e509a7.arrow\n",
      "Loading cached processed dataset at /home/ahora/.cache/huggingface/datasets/csv/default-4caa2e3dddaa112f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-be1143ecb1d9ec25.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda samples: tokenizer(samples['prediction'] , max_length=2048 , truncation=True), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'Abstract', 'Relations', 'prediction', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 469\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'Abstract', 'Relations', 'prediction', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 191\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['Unnamed: 0', 'Abstract', 'Relations', 'prediction', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 41\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 375, 47037, 11759, 24, 8642, 274, 802, 6192, 204, 19, 12849, 23, 560, 246, 33550, 20, 724, 413, 267, 41367, 1334, 635, 2132, 335, 13220, 24, 54587, 7836, 25, 19705, 7016, 335, 301, 798, 1951, 22241, 2383, 241, 28431, 47037, 11759, 336, 12709, 252, 808, 4859, 314, 8285, 7790, 312, 3941, 275, 30194, 2874, 273, 26, 252, 9673, 809, 3968, 10006, 494, 585, 724, 4634, 7686, 10280, 23, 18362, 1419, 23, 379, 1106, 2240, 30194, 2874, 25, 50666, 6443, 60032, 724, 614, 413, 267, 41367, 1334, 635, 2132, 335, 301, 798, 1951, 22241, 204, 25, 26455, 60032, 724, 26701, 375, 248, 33346, 24841, 585, 418, 1122, 248, 15204, 275, 514, 245, 32720, 25, 972, 248, 847, 6192, 362, 739, 34906, 1263, 23, 248, 13220, 50602, 808, 314, 33309, 1988, 1522, 996, 248, 28237, 15204, 275, 514, 245, 32720, 25, 972, 15951, 514, 245, 32720, 431, 13220, 24, 8575, 246, 6030, 23, 248, 9705, 275, 13220, 60032, 808, 314, 16648, 312, 1988, 1522, 852, 514, 245, 32720, 6354, 504, 5954, 25, 1412, 994, 25023, 745, 275, 11253, 353, 1205, 242, 33390, 555, 62516, 6192, 23, 23679, 23, 730, 279, 728, 29401, 23, 724, 9830, 248, 30194, 2547, 3524, 275, 13220, 60032, 25, 4639, 313, 27853, 8768, 275, 59110, 51290, 273, 53174, 304, 3991, 25, 4745, 427, 1988, 4048, 23, 30139, 23, 39006, 52, 24, 5292, 23, 35335, 24, 29, 23, 3250, 441, 416, 2352, 607, 6077, 9482, 1192, 53174, 273, 13220, 60032, 272, 248, 13241, 40004, 56662, 4438, 25, 3249, 2290, 13220, 60032, 23, 2634, 335, 241, 2488, 275, 267, 48128, 19607, 7678, 271, 241, 3451, 275, 53897, 724, 413, 241, 520, 6362, 7678, 313, 8554, 4671, 23, 2335, 31229, 23, 16509, 379, 16409, 25, 8956, 2634, 724, 314, 476, 47410, 271, 248, 7007, 16641, 275, 2957, 438, 50949, 1042, 271, 1796, 248, 23648, 7678, 25, 204, 1579, 37, 258, 19, 560, 246, 33550, 204, 23, 1334, 74, 47, 10211, 204, 23, 13220, 24, 54587, 6553, 204, 20, 204, 24, 258, 19, 50666, 6443, 60032, 204, 23, 1334, 74, 47, 10211, 204, 23, 301, 798, 1951, 22241, 204, 20, 204, 24, 258, 19, 26455, 60032, 204, 23, 1334, 74, 47, 10211, 204, 23, 514, 245, 32720, 204, 20, 204, 24, 258, 19, 13220, 50602, 204, 23, 14103, 74, 47, 10211, 204, 23, 514, 245, 32720, 204, 20, 204, 24, 258, 19, 13220, 60032, 204, 23, 14103, 74, 47, 10211, 204, 23, 514, 245, 32720, 204, 20, 204, 24, 258, 19, 730, 279, 728, 29401, 204, 23, 1334, 74, 47, 10211, 204, 23, 13220, 60032, 204, 20]\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# needed for gpt-neo-x tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    # eval_dataset = dataset['valid'] , \n",
    "    args=transformers.TrainingArguments(\n",
    "        do_eval = False , \n",
    "        auto_find_batch_size = True , \n",
    "        # per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=50,\n",
    "        num_train_epochs=10,\n",
    "        learning_rate=2e-4,\n",
    "        bf16=True,\n",
    "        logging_steps=10,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\" , \n",
    "        dataloader_drop_last= True , \n",
    "        # eval_steps = 100\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e40b6312664dcbbed5c69005c353ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82852b66b05a4467875827420ec5b950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a995bc9be1494d29ad1ed5a9afce1b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6708, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.17}\n",
      "{'loss': 1.4578, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.34}\n",
      "{'loss': 1.2144, 'learning_rate': 0.000132, 'epoch': 0.51}\n",
      "{'loss': 1.2381, 'learning_rate': 0.000172, 'epoch': 0.68}\n",
      "{'loss': 1.1776, 'learning_rate': 0.00019333333333333333, 'epoch': 0.85}\n",
      "{'loss': 1.1932, 'learning_rate': 0.0001711111111111111, 'epoch': 1.03}\n",
      "{'loss': 1.0665, 'learning_rate': 0.0001488888888888889, 'epoch': 1.2}\n",
      "{'loss': 1.0587, 'learning_rate': 0.00012666666666666666, 'epoch': 1.37}\n",
      "{'loss': 1.0676, 'learning_rate': 0.00010444444444444445, 'epoch': 1.54}\n",
      "{'loss': 1.1134, 'learning_rate': 8.222222222222222e-05, 'epoch': 1.71}\n",
      "{'loss': 1.0081, 'learning_rate': 6e-05, 'epoch': 1.88}\n",
      "{'loss': 1.0023, 'learning_rate': 3.777777777777778e-05, 'epoch': 2.05}\n",
      "{'loss': 0.9774, 'learning_rate': 1.5555555555555555e-05, 'epoch': 2.22}\n",
      "{'loss': 0.8137, 'learning_rate': 0.0, 'epoch': 2.39}\n",
      "{'loss': 0.8324, 'learning_rate': 0.0, 'epoch': 2.56}\n",
      "{'loss': 0.8834, 'learning_rate': 0.0, 'epoch': 2.74}\n",
      "{'loss': 0.9135, 'learning_rate': 0.0, 'epoch': 2.91}\n",
      "{'loss': 0.8564, 'learning_rate': 0.0, 'epoch': 3.08}\n",
      "{'loss': 0.9013, 'learning_rate': 0.0, 'epoch': 3.25}\n",
      "{'loss': 0.8544, 'learning_rate': 0.0, 'epoch': 3.42}\n",
      "{'loss': 0.8005, 'learning_rate': 0.0, 'epoch': 3.59}\n",
      "{'loss': 0.8335, 'learning_rate': 0.0, 'epoch': 3.76}\n",
      "{'loss': 0.9269, 'learning_rate': 0.0, 'epoch': 3.93}\n",
      "{'loss': 0.8119, 'learning_rate': 0.0, 'epoch': 4.1}\n",
      "{'loss': 0.8276, 'learning_rate': 0.0, 'epoch': 4.27}\n",
      "{'loss': 0.8909, 'learning_rate': 0.0, 'epoch': 4.44}\n",
      "{'loss': 0.8219, 'learning_rate': 0.0, 'epoch': 4.62}\n",
      "{'loss': 0.9094, 'learning_rate': 0.0, 'epoch': 4.79}\n",
      "{'loss': 0.8247, 'learning_rate': 0.0, 'epoch': 4.96}\n",
      "{'loss': 0.8556, 'learning_rate': 0.0, 'epoch': 5.13}\n",
      "{'loss': 0.8647, 'learning_rate': 0.0, 'epoch': 5.3}\n",
      "{'loss': 0.8599, 'learning_rate': 0.0, 'epoch': 5.47}\n",
      "{'loss': 0.8576, 'learning_rate': 0.0, 'epoch': 5.64}\n",
      "{'loss': 0.8239, 'learning_rate': 0.0, 'epoch': 5.81}\n",
      "{'loss': 0.8638, 'learning_rate': 0.0, 'epoch': 5.98}\n",
      "{'loss': 0.8569, 'learning_rate': 0.0, 'epoch': 6.15}\n",
      "{'loss': 0.8297, 'learning_rate': 0.0, 'epoch': 6.32}\n",
      "{'loss': 0.8331, 'learning_rate': 0.0, 'epoch': 6.5}\n",
      "{'loss': 0.8645, 'learning_rate': 0.0, 'epoch': 6.67}\n",
      "{'loss': 0.8466, 'learning_rate': 0.0, 'epoch': 6.84}\n",
      "{'loss': 0.9026, 'learning_rate': 0.0, 'epoch': 7.01}\n",
      "{'loss': 0.8709, 'learning_rate': 0.0, 'epoch': 7.18}\n",
      "{'loss': 0.8629, 'learning_rate': 0.0, 'epoch': 7.35}\n",
      "{'loss': 0.805, 'learning_rate': 0.0, 'epoch': 7.52}\n",
      "{'loss': 0.8786, 'learning_rate': 0.0, 'epoch': 7.69}\n",
      "{'loss': 0.8457, 'learning_rate': 0.0, 'epoch': 7.86}\n",
      "{'loss': 0.848, 'learning_rate': 0.0, 'epoch': 8.03}\n",
      "{'loss': 0.8031, 'learning_rate': 0.0, 'epoch': 8.21}\n",
      "{'loss': 0.8491, 'learning_rate': 0.0, 'epoch': 8.38}\n",
      "{'loss': 0.8982, 'learning_rate': 0.0, 'epoch': 8.55}\n",
      "{'loss': 0.9532, 'learning_rate': 0.0, 'epoch': 8.72}\n",
      "{'loss': 0.8039, 'learning_rate': 0.0, 'epoch': 8.89}\n",
      "{'loss': 0.8259, 'learning_rate': 0.0, 'epoch': 9.06}\n",
      "{'loss': 0.8571, 'learning_rate': 0.0, 'epoch': 9.23}\n",
      "{'loss': 0.8454, 'learning_rate': 0.0, 'epoch': 9.4}\n",
      "{'loss': 0.8295, 'learning_rate': 0.0, 'epoch': 9.57}\n",
      "{'loss': 0.8416, 'learning_rate': 0.0, 'epoch': 9.74}\n",
      "{'loss': 0.8619, 'learning_rate': 0.0, 'epoch': 9.91}\n",
      "{'train_runtime': 4996.7766, 'train_samples_per_second': 0.939, 'train_steps_per_second': 0.116, 'train_loss': 0.9256400059009421, 'epoch': 9.91}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=580, training_loss=0.9256400059009421, metrics={'train_runtime': 4996.7766, 'train_samples_per_second': 0.939, 'train_steps_per_second': 0.116, 'train_loss': 0.9256400059009421, 'epoch': 9.91})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./Model_Saves/tokenizer_config.json',\n",
       " './Model_Saves/special_tokens_map.json',\n",
       " './Model_Saves/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = \"./Model_Saves\"\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"Although clinical studies have not established a cause and effect relationship, physicians should be aware that variable effects an blood coagulation have been reported very rarely in patients receiving oral anticoagulants and chlordiazepoxide. The concomitant use of alcohol or other central nervous system depressants may have an additive effect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus(test_prompt, return_tensors=\"pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = inputs[\"input_ids\"].to('cuda')\n",
    "attention_mask = inputs[\"attention_mask\"].to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "/home/ahora/anaconda3/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids =input_ids , attention_mask = attention_mask , max_length=512, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8127,  6077,  4048,   413,   416,  5305,   241,  2887,   273,  1334,\n",
       "         2392,    23, 16923,   808,   314,  3671,   325,  9647,  3524,   267,\n",
       "         2902,   739, 51627,   413,   650,  3713,   829, 10803,   272,  2634,\n",
       "         7537, 10720, 54788,   353,   338,  1221,   273,   425, 42322, 19383,\n",
       "          538, 24862,    25,   390, 59819, 25023,   745,   275,  7185,   379,\n",
       "          599,  5213, 10886,  1092, 51929,  1221,   724,   413,   267, 41367,\n",
       "         1334,    25,   204,  1579,    37,   258,    19,   425, 42322, 19383,\n",
       "          538, 24862,   204,    23,  1334,    74,    47, 10211,   204,    23,\n",
       "        54788,   353,   338,  1221,   204,    20,   204,    24,   258,    19,\n",
       "          425, 42322, 19383,   538, 24862,   204,    23,  1334,    74,    47,\n",
       "        10211,   204,    23,  5213, 10886,  1092, 51929,  1221,   204,    20,\n",
       "          204,    24,   258,    19,  7185,   204,    23,  1334,    74,    47,\n",
       "        10211,   204,    23,   425, 42322, 19383,   538, 24862,   204,    20,\n",
       "          204,    24,   258,    19,  7185,   204,    23,  1334,    74,    47,\n",
       "        10211,   204,    23,  5213, 10886,  1092, 51929,  1221,   204,    20,\n",
       "          204,    24,   258,    19,  5213, 10886,  1092, 51929,  1221,   204,\n",
       "           23,  1334,    74,    47, 10211,   204,    23,   425, 42322, 19383,\n",
       "          538, 24862,   204,    20,   204,    24,   258,    19,  7185,   204,\n",
       "           23,  1334,    74,    47, 10211,   204,    23, 54788,   353,   338,\n",
       "         1221,   204,    20,   204,    24,   258,    19,  7185,   204,    23,\n",
       "         1334,    74,    47, 10211,   204,    23,  5213, 10886,  1092, 51929,\n",
       "         1221,   204,    20,   204,    24,   258,    19,  7185,   204,    23,\n",
       "         1334,    74,    47, 10211,   204,    23,   425, 42322, 19383,   538,\n",
       "        24862,   204,    20,   204,    24,   258,    19,  7185,   204,    23,\n",
       "         1334,    74,    47, 10211,   204,    23, 54788,   353,   338,  1221,\n",
       "          204,    20,   204,    24,   258,    19,  7185,   204,    23,  1334,\n",
       "           74,    47, 10211,   204,    23,  5213, 10886,  1092, 51929,  1221,\n",
       "          204,    20,   204,    24,   258,    19,  7185,   204,    23,  1334,\n",
       "           74,    47, 10211,   204,    23,   425, 42322, 19383,   538, 24862,\n",
       "          204,    20,   204,    24,   258,    19,  5213, 10886,  1092, 51929,\n",
       "         1221,   204,    23,  1334,    74,    47, 10211,   204,    23, 54788,\n",
       "          353,   338,  1221,   204,    20,   204,    24,   258,    19,  5213,\n",
       "        10886,  1092, 51929,  1221,   204,    23,  1334,    74,    47, 10211,\n",
       "          204,    23,   425, 42322, 19383,   538, 24862,   204,    20,   204,\n",
       "           24,   258,    19,  7185,   204,    23,  1334,    74,    47, 10211,\n",
       "          204,    23, 54788,   353,   338,  1221,   204,    20,   204,    24,\n",
       "          258,    19,  7185,   204,    23,  1334,    74,    47, 10211,   204,\n",
       "           23,   425, 42322, 19383,   538, 24862,   204,    20,   204,    24,\n",
       "          258,    19,  7185,   204,    23,  1334,    74,    47, 10211,   204,\n",
       "           23,  5213, 10886,  1092, 51929,  1221,   204,    20,   204,    24,\n",
       "          258,    19,  7185,   204,    23,  1334,    74,    47, 10211,   204,\n",
       "           23,   425, 42322, 19383,   538, 24862,   204,    20,   204,    24,\n",
       "          258,    19,  7185,   204,    23,  1334,    74,    47, 10211,   204,\n",
       "           23, 54788,   353,   338,  1221,   204,    20,   204,    24,   258,\n",
       "           19,  7185,   204,    23,  1334,    74,    47, 10211,   204,    23,\n",
       "         5213, 10886,  1092, 51929,  1221,   204,    20,   204,    24,   258,\n",
       "           19,  7185,   204,    23,  1334,    74,    47, 10211,   204,    23,\n",
       "          425, 42322, 19383,   538, 24862,   204,    20,   204,    24,   258,\n",
       "           19,  7185,   204,    23,  1334,    74,    47, 10211,   204,    23,\n",
       "        54788,   353,   338,  1221,   204,    20,   204,    24,   258,    19,\n",
       "         7185,   204,    23,  1334,    74,    47, 10211,   204,    23,  5213,\n",
       "        10886,  1092], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Although clinical studies have not established a cause and effect relationship, physicians should be aware that variable effects an blood coagulation have been reported very rarely in patients receiving oral anticoagulants and chlordiazepoxide. The concomitant use of alcohol or other central nervous system depressants may have an additive effect. ->:  ( chlordiazepoxide, effect_DDI, anticoagulants ) -  ( chlordiazepoxide, effect_DDI, central nervous system depressants ) -  ( alcohol, effect_DDI, chlordiazepoxide ) -  ( alcohol, effect_DDI, central nervous system depressants ) -  ( central nervous system depressants, effect_DDI, chlordiazepoxide ) -  ( alcohol, effect_DDI, anticoagulants ) -  ( alcohol, effect_DDI, central nervous system depressants ) -  ( alcohol, effect_DDI, chlordiazepoxide ) -  ( alcohol, effect_DDI, anticoagulants ) -  ( alcohol, effect_DDI, central nervous system depressants ) -  ( alcohol, effect_DDI, chlordiazepoxide ) -  ( central nervous system depressants, effect_DDI, anticoagulants ) -  ( central nervous system depressants, effect_DDI, chlordiazepoxide ) -  ( alcohol, effect_DDI, anticoagulants ) -  ( alcohol, effect_DDI, chlordiazepoxide ) -  ( alcohol, effect_DDI, central nervous system depressants ) -  ( alcohol, effect_DDI, chlordiazepoxide ) -  ( alcohol, effect_DDI, anticoagulants ) -  ( alcohol, effect_DDI, central nervous system depressants ) -  ( alcohol, effect_DDI, chlordiazepoxide ) -  ( alcohol, effect_DDI, anticoagulants ) -  ( alcohol, effect_DDI, central nervous system\n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(output[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
